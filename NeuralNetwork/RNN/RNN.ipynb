{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN 递归神经网络\n",
    "\n",
    "\n",
    "## RNN概述\n",
    "    \n",
    "    在全连接神经网络或卷积神经网络中，网络结果都是从输入层到隐含层再到输出层，层与层之间是全连接或部分连接的，但每层之间的结点是无连接的。在解决输入是序列，需要“记忆”来结局的问题时就无能为力了。例如：给出一个句子，给出前面几个单词，预测下一个单词是什么。这时一般需要用到当前单词以及前面的单词，因为句子中前后单词并不是独立的，比如，当前单词是“很”，前一个单词是“天空”，那么下一个单词很大概率是“蓝”。 这时，循环神经网络(Recurrent Neural Networks，简称RNN)就有了用武之地。 RNN通常应用于解决训练样本输入是连续的序列, 且序列的长短不一的问题，它刻画一个序列当前的输出与之前信息的关系。从网络结果上来说，RNN会记忆之前的信息，并利用之前的信息影响后面的输出。也就是说，RNN的隐藏层之间的结点是有连接的，隐藏层的输入不仅包括输入层的输出，还包含上一时刻隐藏层的输出。\n",
    "    \n",
    "![RNN_8.png](RNN_8.jpeg)\n",
    "    当前，它被广泛的用于自然语言处理中的语言建模和文本生成，语音识别，以及机器翻译等领域。\n",
    "    例如：股票预测中的RNN，输入是前N天价格，输出明天的股市价格。\n",
    "    \n",
    "    \n",
    "## RNN的结构：\n",
    "\n",
    "![RNN_3.png](RNN_3.png)\n",
    "\n",
    "上图描述了经典的RNN模型，具体的参数解释如下：\n",
    "\n",
    "对于序列索引号t，其中：\n",
    "1. x(t)代表在序列索引号t时训练样本的输入。同样的，x(t−1)和x(t+1)代表在序列索引号t−1和t+1时训练样本的输入。\n",
    "2. h(t)代表在序列索引号t时模型的隐藏状态，直观的理解为“记忆”。h(t)由x(t)和h(t−1)共同决定。 计算公式如下: h(t) = f(U * x(t) + W * h(t-1))\n",
    "3. o(t)代表在序列索引号t时模型的输出。o(t)只由模型当前的隐藏状态h(t)决定。\n",
    "4. L(t)代表在序列索引号t时模型的损失函数。\n",
    "5. y(t)代表在序列索引号t时训练样本序列的真实输出。\n",
    "6. U,W,V这三个矩阵是我们的模型的线性关系参数，它在整个RNN网络中是共享的，这点和DNN很不相同。也正因为是共享了，体现了RNN的模型的“循环反馈”的思想。同时，这也意味着这个模型对于每一步的作用是一致的，只是输入不同。这样的方式大幅降低了需要学习的参数总数，减少了很多计算量。\n",
    "\n",
    "\n",
    "\n",
    "## RNN主要层次示例\n",
    "\n",
    "### 应用CNN进行汽车图像识别用例\n",
    "![RNN_1.png](RNN_1.png)\n",
    "\n",
    "\n",
    "### 通过卷积计算进行局部特征提取\n",
    "![RNN_2.gif](RNN_2.gif)\n",
    "![RNN_3.png](CNN_3.png)\n",
    "![RNN_4.png](RNN_4.jpeg)\n",
    "![RNN_5.png](RNN_5.gif)\n",
    "![RNN_6.gif](RNN_6.gif)\n",
    "![RNN_8.png](RNN_8.jpeg)\n",
    "\n",
    "\n",
    "\n",
    "### 池化示例（最大化池化）\n",
    "\n",
    "\n",
    "\n",
    "### RNN算法汇总\n",
    "![CNN_7.png](CNN_7.jpeg)\n",
    "\n",
    "\n",
    "### RNN的经典论文\n",
    "- Recurrent neural network based language model 《基于循环神经网络的语言模型》\n",
    "- Extensions of Recurrent neural network based language model《基于循环神经网络拓展的语言模型》\n",
    "- Generating Text with Recurrent Neural Networks《利用循环神经网络生成文本》\n",
    "- A Recursive Recurrent Neural Network for Statistical Machine Translation《用于统计类机器翻译的递归型循环神经网络》\n",
    "- Sequence to Sequence Learning with Neural Networks《利用神经网络进行序列至序列的学习》\n",
    "- Joint Language and Translation Modeling with Recurrent Neural Networks《利用循环神经网络进行语言和翻译的建模》\n",
    "- Towards End-to-End Speech Recognition with Recurrent Neural Networks《利用循环神经网络进行端对端的语音识别》"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 例1. 基于TensorFlow，搭建LSTM-RNN模型，教会神经网络进行二进制加法\n",
    "![RNN_7.png](RNN_7.gif)\n",
    "\n",
    "参考:  [Anyone Can learn To Code LSTM-RNN in Python(Part 1: RNN)](https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(\"tensorflow version: \"+str(tf.__version__))\n",
    "\n",
    "\n",
    "# 一个字典，隐射一个数字到其二进制的表示\n",
    "# 例如 int2binary[3] = [0,0,0,0,0,0,1,1]\n",
    "int2binary = {}\n",
    "\n",
    "# 最多8位二进制\n",
    "binary_dim = 8\n",
    "\n",
    "# 在8位情况下，最大数为2^8 = 256\n",
    "largest_number = pow(2,binary_dim)\n",
    "\n",
    "# 将[0,256)所有数表示成二进制\n",
    "binary = np.unpackbits(\n",
    "    np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
    "\n",
    "# 建立字典\n",
    "for i in range(largest_number):\n",
    "    int2binary[i] = binary[i]\n",
    "\n",
    "def binary_generation(numbers, reverse = False):\n",
    "    '''\n",
    "    返回numbers中所有数的二进制表达，\n",
    "    例如 numbers = [3, 2, 1]\n",
    "    返回 ：[[0,0,0,0,0,0,1,1],\n",
    "            [0,0,0,0,0,0,1,0],\n",
    "            [0,0,0,0,0,0,0,1]'\n",
    "            \n",
    "    如果 reverse = True, 二进制表达式前后颠倒，\n",
    "    这么做是为训练方便，因为训练的输入顺序是从低位开始的\n",
    "    \n",
    "    numbers : 一组数字\n",
    "    reverse : 是否将其二进制表示进行前后翻转\n",
    "    '''\n",
    "    binary_x = np.array([ int2binary[num] for num in numbers], dtype=np.uint8)\n",
    "    \n",
    "    if reverse:\n",
    "        binary_x = np.fliplr(binary_x)\n",
    "    \n",
    "    return binary_x\n",
    "\n",
    "def batch_generation(batch_size, largest_number):\n",
    "    '''\n",
    "    生成batch_size大小的数据，用于训练或者验证\n",
    "    \n",
    "    batch_x 大小为[batch_size, biniary_dim, 2]\n",
    "    batch_y 大小为[batch_size, biniray_dim]\n",
    "    '''\n",
    "\n",
    "    # 随机生成batch_size个数\n",
    "    n1 = np.random.randint(0, largest_number//2, batch_size)\n",
    "    n2 = np.random.randint(0, largest_number//2, batch_size)\n",
    "    # 计算加法结果\n",
    "    add = n1 + n2\n",
    "    \n",
    "    # int to binary\n",
    "    binary_n1 = binary_generation(n1, True)\n",
    "    binary_n2 = binary_generation(n2, True)\n",
    "    batch_y = binary_generation(add, True)\n",
    "    \n",
    "    # 堆叠，因为网络的输入是2个二进制\n",
    "    batch_x = np.dstack((binary_n1, binary_n2))\n",
    "    \n",
    "    return batch_x, batch_y, n1, n2, add\n",
    "\n",
    "def binary2int(binary_array):\n",
    "    '''\n",
    "    将一个二进制数组转为整数\n",
    "    '''\n",
    "    out = 0\n",
    "    for index, x in enumerate(reversed(binary_array)):\n",
    "        out += x*pow(2, index)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# LSTM的个数，就是隐层中神经元的数量\n",
    "lstm_size = 20\n",
    "\n",
    "# 隐层的层数\n",
    "lstm_layers =2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义输入输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入，[None, binary_dim, 2], \n",
    "# None表示batch_size, binary_dim表示输入序列的长度，2表示每个时刻有两个输入\n",
    "x = tf.placeholder(tf.float32, [None, binary_dim, 2], name='input_x')\n",
    "\n",
    "# 输出\n",
    "y_ = tf.placeholder(tf.float32, [None, binary_dim], name='input_y')\n",
    "# dropout 参数\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-4-a2a978a6716c>\", line 16, in <module>\n    outputs, final_state = tf.nn.dynamic_rnn(cell, x, initial_state=initial_state)\n  File \"/Users/liang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/Users/liang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-7bece603d08c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# 进行forward，得到隐层的输出\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# outputs 大小为[batch_size, lstm_size*binary_dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# 建立输出层\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36mdynamic_rnn\u001b[0;34m(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[1;32m    625\u001b[0m         \u001b[0mswap_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswap_memory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m         \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m         dtype=dtype)\n\u001b[0m\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m     \u001b[0;31m# Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36m_dynamic_rnn_loop\u001b[0;34m(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)\u001b[0m\n\u001b[1;32m    822\u001b[0m       \u001b[0mparallel_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m       \u001b[0mmaximum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m       swap_memory=swap_memory)\n\u001b[0m\u001b[1;32m    825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m   \u001b[0;31m# Unpack final output if not using output tuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations)\u001b[0m\n\u001b[1;32m   3222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mloop_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mouter_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3223\u001b[0m       \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWHILE_CONTEXT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3224\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloop_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBuildLoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape_invariants\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3225\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmaximum_iterations\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3226\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mBuildLoop\u001b[0;34m(self, pred, body, loop_vars, shape_invariants)\u001b[0m\n\u001b[1;32m   2954\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2955\u001b[0m         original_body_result, exit_vars = self._BuildLoop(\n\u001b[0;32m-> 2956\u001b[0;31m             pred, body, original_loop_vars, loop_vars, shape_invariants)\n\u001b[0m\u001b[1;32m   2957\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2958\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36m_BuildLoop\u001b[0;34m(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\u001b[0m\n\u001b[1;32m   2891\u001b[0m         flat_sequence=vars_for_body_with_tensor_arrays)\n\u001b[1;32m   2892\u001b[0m     \u001b[0mpre_summaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2893\u001b[0;31m     \u001b[0mbody_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpacked_vars_for_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2894\u001b[0m     \u001b[0mpost_summaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2895\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i, lv)\u001b[0m\n\u001b[1;32m   3192\u001b[0m         cond = lambda i, lv: (  # pylint: disable=g-long-lambda\n\u001b[1;32m   3193\u001b[0m             math_ops.logical_and(i < maximum_iterations, orig_cond(*lv)))\n\u001b[0;32m-> 3194\u001b[0;31m         \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3196\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36m_time_step\u001b[0;34m(time, output_ta_t, state)\u001b[0m\n\u001b[1;32m    793\u001b[0m           skip_conditionals=True)\n\u001b[1;32m    794\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m       \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m     \u001b[0;31m# Pack state if using state tuples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m     \u001b[0minput_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_sequence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 781\u001b[0;31m     \u001b[0mcall_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, state, scope)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope_attrname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNNCell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_rnn_get_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0min_deferred_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m             raise ValueError('A layer\\'s `call` method should return a Tensor '\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, state)\u001b[0m\n\u001b[1;32m   1290\u001b[0m                                       [-1, cell.state_size])\n\u001b[1;32m   1291\u001b[0m           \u001b[0mcur_state_pos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m         \u001b[0mcur_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m         \u001b[0mnew_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, state, scope, *args, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0;31m# method.  See the class docstring for more details.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     return base_layer.Layer.__call__(self, inputs, state, scope=scope,\n\u001b[0;32m--> 339\u001b[0;31m                                      *args, **kwargs)\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    697\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'get_shape'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0minput_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m           \u001b[0;31m# Note: not all sub-classes of Layer call Layer.__init__ (especially\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, inputs_shape)\u001b[0m\n\u001b[1;32m    586\u001b[0m     self._kernel = self.add_variable(\n\u001b[1;32m    587\u001b[0m         \u001b[0m_WEIGHTS_VARIABLE_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         shape=[input_depth + h_depth, 4 * self._num_units])\n\u001b[0m\u001b[1;32m    589\u001b[0m     self._bias = self.add_variable(\n\u001b[1;32m    590\u001b[0m         \u001b[0m_BIAS_VARIABLE_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36madd_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner)\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m             partitioner=partitioner)\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minit_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/checkpointable.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    434\u001b[0m     new_variable = getter(\n\u001b[1;32m    435\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m         **kwargs_for_getter)\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[0;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1315\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m       constraint=constraint)\n\u001b[0m\u001b[1;32m   1318\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1319\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1077\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m    415\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m\"constraint\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mestimator_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0mcustom_getter_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"constraint\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstraint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcustom_getter_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m       return _true_getter(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\u001b[0m in \u001b[0;36m_rnn_get_variable\u001b[0;34m(self, getter, *args, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_rnn_get_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m     \u001b[0mvariable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainable\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    392\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    731\u001b[0m                          \u001b[0;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 733\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    734\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-4-a2a978a6716c>\", line 16, in <module>\n    outputs, final_state = tf.nn.dynamic_rnn(cell, x, initial_state=initial_state)\n  File \"/Users/liang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/Users/liang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 搭建LSTM层（看成隐层）\n",
    "# 有lstm_size个单元\n",
    "lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "# dropout\n",
    "drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "# 一层不够，就多来几层\n",
    "def lstm_cell():\n",
    "    return tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "\n",
    "cell = tf.contrib.rnn.MultiRNNCell([ lstm_cell() for _ in range(lstm_layers)])\n",
    "\n",
    "# 初始状态，可以理解为初始记忆\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "# 进行forward，得到隐层的输出\n",
    "# outputs 大小为[batch_size, lstm_size*binary_dim]\n",
    "outputs, final_state = tf.nn.dynamic_rnn(cell, x, initial_state=initial_state)\n",
    "\n",
    "# 建立输出层\n",
    "weights = tf.Variable(tf.truncated_normal([lstm_size, 1], stddev=0.01))\n",
    "bias = tf.zeros([1])\n",
    "\n",
    "# [batch_size, lstm_size*binary_dim] ==> [batch_size*binary_dim, lstm_size]\n",
    "outputs = tf.reshape(outputs, [-1, lstm_size])\n",
    "# 得到输出, logits大小为[batch_size*binary_dim, 1]\n",
    "logits = tf.sigmoid(tf.matmul(outputs, weights))\n",
    "# [batch_size*binary_dim, 1] ==> [batch_size, binary_dim]\n",
    "predictions = tf.reshape(logits, [-1, binary_dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数和优化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.losses.mean_squared_error(y_, predictions)\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 2000\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    iteration = 1\n",
    "    for i in range(steps):\n",
    "        # 获取训练数据\n",
    "        input_x, input_y,_,_,_ = batch_generation(batch_size, largest_number)\n",
    "        _, loss = sess.run([optimizer, cost], feed_dict={x:input_x, y_:input_y, keep_prob:0.5})\n",
    "        \n",
    "        if iteration % 1000 == 0:\n",
    "            print('Iter:{}, Loss:{}'.format(iteration, loss))    \n",
    "        iteration += 1\n",
    "    \n",
    "    # 训练结束，进行测试\n",
    "    val_x, val_y, n1, n2, add = batch_generation(batch_size, largest_number)\n",
    "    result = sess.run(predictions, feed_dict={x:val_x, y_:val_y, keep_prob:1.0})\n",
    "            \n",
    "    # 左右翻转二进制数组。因为输出的结果是低位在前，而正常的表达是高位在前，因此进行翻转\n",
    "    result = np.fliplr(np.round(result))\n",
    "    result = result.astype(np.int32)\n",
    "            \n",
    "    for  b_x, b_p, a, b, add in zip(np.fliplr(val_x), result, n1, n2, add):\n",
    "        print('{}:{}'.format(b_x[:,0], a))\n",
    "        print('{}:{}'.format(b_x[:,1], b))\n",
    "        print('{}:{}\\n'.format(b_p, binary2int(b_p)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上例使用python实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---binary---\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 1 0]\n",
      " ...\n",
      " [1 1 1 ... 1 0 1]\n",
      " [1 1 1 ... 1 1 0]\n",
      " [1 1 1 ... 1 1 1]]\n",
      "\n",
      "\n",
      "---int2binary---\n",
      "{0: array([0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8), 1: array([0, 0, 0, 0, 0, 0, 0, 1], dtype=uint8), 2: array([0, 0, 0, 0, 0, 0, 1, 0], dtype=uint8), 3: array([0, 0, 0, 0, 0, 0, 1, 1], dtype=uint8), 4: array([0, 0, 0, 0, 0, 1, 0, 0], dtype=uint8), 5: array([0, 0, 0, 0, 0, 1, 0, 1], dtype=uint8), 6: array([0, 0, 0, 0, 0, 1, 1, 0], dtype=uint8), 7: array([0, 0, 0, 0, 0, 1, 1, 1], dtype=uint8), 8: array([0, 0, 0, 0, 1, 0, 0, 0], dtype=uint8), 9: array([0, 0, 0, 0, 1, 0, 0, 1], dtype=uint8), 10: array([0, 0, 0, 0, 1, 0, 1, 0], dtype=uint8), 11: array([0, 0, 0, 0, 1, 0, 1, 1], dtype=uint8), 12: array([0, 0, 0, 0, 1, 1, 0, 0], dtype=uint8), 13: array([0, 0, 0, 0, 1, 1, 0, 1], dtype=uint8), 14: array([0, 0, 0, 0, 1, 1, 1, 0], dtype=uint8), 15: array([0, 0, 0, 0, 1, 1, 1, 1], dtype=uint8), 16: array([0, 0, 0, 1, 0, 0, 0, 0], dtype=uint8), 17: array([0, 0, 0, 1, 0, 0, 0, 1], dtype=uint8), 18: array([0, 0, 0, 1, 0, 0, 1, 0], dtype=uint8), 19: array([0, 0, 0, 1, 0, 0, 1, 1], dtype=uint8), 20: array([0, 0, 0, 1, 0, 1, 0, 0], dtype=uint8), 21: array([0, 0, 0, 1, 0, 1, 0, 1], dtype=uint8), 22: array([0, 0, 0, 1, 0, 1, 1, 0], dtype=uint8), 23: array([0, 0, 0, 1, 0, 1, 1, 1], dtype=uint8), 24: array([0, 0, 0, 1, 1, 0, 0, 0], dtype=uint8), 25: array([0, 0, 0, 1, 1, 0, 0, 1], dtype=uint8), 26: array([0, 0, 0, 1, 1, 0, 1, 0], dtype=uint8), 27: array([0, 0, 0, 1, 1, 0, 1, 1], dtype=uint8), 28: array([0, 0, 0, 1, 1, 1, 0, 0], dtype=uint8), 29: array([0, 0, 0, 1, 1, 1, 0, 1], dtype=uint8), 30: array([0, 0, 0, 1, 1, 1, 1, 0], dtype=uint8), 31: array([0, 0, 0, 1, 1, 1, 1, 1], dtype=uint8), 32: array([0, 0, 1, 0, 0, 0, 0, 0], dtype=uint8), 33: array([0, 0, 1, 0, 0, 0, 0, 1], dtype=uint8), 34: array([0, 0, 1, 0, 0, 0, 1, 0], dtype=uint8), 35: array([0, 0, 1, 0, 0, 0, 1, 1], dtype=uint8), 36: array([0, 0, 1, 0, 0, 1, 0, 0], dtype=uint8), 37: array([0, 0, 1, 0, 0, 1, 0, 1], dtype=uint8), 38: array([0, 0, 1, 0, 0, 1, 1, 0], dtype=uint8), 39: array([0, 0, 1, 0, 0, 1, 1, 1], dtype=uint8), 40: array([0, 0, 1, 0, 1, 0, 0, 0], dtype=uint8), 41: array([0, 0, 1, 0, 1, 0, 0, 1], dtype=uint8), 42: array([0, 0, 1, 0, 1, 0, 1, 0], dtype=uint8), 43: array([0, 0, 1, 0, 1, 0, 1, 1], dtype=uint8), 44: array([0, 0, 1, 0, 1, 1, 0, 0], dtype=uint8), 45: array([0, 0, 1, 0, 1, 1, 0, 1], dtype=uint8), 46: array([0, 0, 1, 0, 1, 1, 1, 0], dtype=uint8), 47: array([0, 0, 1, 0, 1, 1, 1, 1], dtype=uint8), 48: array([0, 0, 1, 1, 0, 0, 0, 0], dtype=uint8), 49: array([0, 0, 1, 1, 0, 0, 0, 1], dtype=uint8), 50: array([0, 0, 1, 1, 0, 0, 1, 0], dtype=uint8), 51: array([0, 0, 1, 1, 0, 0, 1, 1], dtype=uint8), 52: array([0, 0, 1, 1, 0, 1, 0, 0], dtype=uint8), 53: array([0, 0, 1, 1, 0, 1, 0, 1], dtype=uint8), 54: array([0, 0, 1, 1, 0, 1, 1, 0], dtype=uint8), 55: array([0, 0, 1, 1, 0, 1, 1, 1], dtype=uint8), 56: array([0, 0, 1, 1, 1, 0, 0, 0], dtype=uint8), 57: array([0, 0, 1, 1, 1, 0, 0, 1], dtype=uint8), 58: array([0, 0, 1, 1, 1, 0, 1, 0], dtype=uint8), 59: array([0, 0, 1, 1, 1, 0, 1, 1], dtype=uint8), 60: array([0, 0, 1, 1, 1, 1, 0, 0], dtype=uint8), 61: array([0, 0, 1, 1, 1, 1, 0, 1], dtype=uint8), 62: array([0, 0, 1, 1, 1, 1, 1, 0], dtype=uint8), 63: array([0, 0, 1, 1, 1, 1, 1, 1], dtype=uint8), 64: array([0, 1, 0, 0, 0, 0, 0, 0], dtype=uint8), 65: array([0, 1, 0, 0, 0, 0, 0, 1], dtype=uint8), 66: array([0, 1, 0, 0, 0, 0, 1, 0], dtype=uint8), 67: array([0, 1, 0, 0, 0, 0, 1, 1], dtype=uint8), 68: array([0, 1, 0, 0, 0, 1, 0, 0], dtype=uint8), 69: array([0, 1, 0, 0, 0, 1, 0, 1], dtype=uint8), 70: array([0, 1, 0, 0, 0, 1, 1, 0], dtype=uint8), 71: array([0, 1, 0, 0, 0, 1, 1, 1], dtype=uint8), 72: array([0, 1, 0, 0, 1, 0, 0, 0], dtype=uint8), 73: array([0, 1, 0, 0, 1, 0, 0, 1], dtype=uint8), 74: array([0, 1, 0, 0, 1, 0, 1, 0], dtype=uint8), 75: array([0, 1, 0, 0, 1, 0, 1, 1], dtype=uint8), 76: array([0, 1, 0, 0, 1, 1, 0, 0], dtype=uint8), 77: array([0, 1, 0, 0, 1, 1, 0, 1], dtype=uint8), 78: array([0, 1, 0, 0, 1, 1, 1, 0], dtype=uint8), 79: array([0, 1, 0, 0, 1, 1, 1, 1], dtype=uint8), 80: array([0, 1, 0, 1, 0, 0, 0, 0], dtype=uint8), 81: array([0, 1, 0, 1, 0, 0, 0, 1], dtype=uint8), 82: array([0, 1, 0, 1, 0, 0, 1, 0], dtype=uint8), 83: array([0, 1, 0, 1, 0, 0, 1, 1], dtype=uint8), 84: array([0, 1, 0, 1, 0, 1, 0, 0], dtype=uint8), 85: array([0, 1, 0, 1, 0, 1, 0, 1], dtype=uint8), 86: array([0, 1, 0, 1, 0, 1, 1, 0], dtype=uint8), 87: array([0, 1, 0, 1, 0, 1, 1, 1], dtype=uint8), 88: array([0, 1, 0, 1, 1, 0, 0, 0], dtype=uint8), 89: array([0, 1, 0, 1, 1, 0, 0, 1], dtype=uint8), 90: array([0, 1, 0, 1, 1, 0, 1, 0], dtype=uint8), 91: array([0, 1, 0, 1, 1, 0, 1, 1], dtype=uint8), 92: array([0, 1, 0, 1, 1, 1, 0, 0], dtype=uint8), 93: array([0, 1, 0, 1, 1, 1, 0, 1], dtype=uint8), 94: array([0, 1, 0, 1, 1, 1, 1, 0], dtype=uint8), 95: array([0, 1, 0, 1, 1, 1, 1, 1], dtype=uint8), 96: array([0, 1, 1, 0, 0, 0, 0, 0], dtype=uint8), 97: array([0, 1, 1, 0, 0, 0, 0, 1], dtype=uint8), 98: array([0, 1, 1, 0, 0, 0, 1, 0], dtype=uint8), 99: array([0, 1, 1, 0, 0, 0, 1, 1], dtype=uint8), 100: array([0, 1, 1, 0, 0, 1, 0, 0], dtype=uint8), 101: array([0, 1, 1, 0, 0, 1, 0, 1], dtype=uint8), 102: array([0, 1, 1, 0, 0, 1, 1, 0], dtype=uint8), 103: array([0, 1, 1, 0, 0, 1, 1, 1], dtype=uint8), 104: array([0, 1, 1, 0, 1, 0, 0, 0], dtype=uint8), 105: array([0, 1, 1, 0, 1, 0, 0, 1], dtype=uint8), 106: array([0, 1, 1, 0, 1, 0, 1, 0], dtype=uint8), 107: array([0, 1, 1, 0, 1, 0, 1, 1], dtype=uint8), 108: array([0, 1, 1, 0, 1, 1, 0, 0], dtype=uint8), 109: array([0, 1, 1, 0, 1, 1, 0, 1], dtype=uint8), 110: array([0, 1, 1, 0, 1, 1, 1, 0], dtype=uint8), 111: array([0, 1, 1, 0, 1, 1, 1, 1], dtype=uint8), 112: array([0, 1, 1, 1, 0, 0, 0, 0], dtype=uint8), 113: array([0, 1, 1, 1, 0, 0, 0, 1], dtype=uint8), 114: array([0, 1, 1, 1, 0, 0, 1, 0], dtype=uint8), 115: array([0, 1, 1, 1, 0, 0, 1, 1], dtype=uint8), 116: array([0, 1, 1, 1, 0, 1, 0, 0], dtype=uint8), 117: array([0, 1, 1, 1, 0, 1, 0, 1], dtype=uint8), 118: array([0, 1, 1, 1, 0, 1, 1, 0], dtype=uint8), 119: array([0, 1, 1, 1, 0, 1, 1, 1], dtype=uint8), 120: array([0, 1, 1, 1, 1, 0, 0, 0], dtype=uint8), 121: array([0, 1, 1, 1, 1, 0, 0, 1], dtype=uint8), 122: array([0, 1, 1, 1, 1, 0, 1, 0], dtype=uint8), 123: array([0, 1, 1, 1, 1, 0, 1, 1], dtype=uint8), 124: array([0, 1, 1, 1, 1, 1, 0, 0], dtype=uint8), 125: array([0, 1, 1, 1, 1, 1, 0, 1], dtype=uint8), 126: array([0, 1, 1, 1, 1, 1, 1, 0], dtype=uint8), 127: array([0, 1, 1, 1, 1, 1, 1, 1], dtype=uint8), 128: array([1, 0, 0, 0, 0, 0, 0, 0], dtype=uint8), 129: array([1, 0, 0, 0, 0, 0, 0, 1], dtype=uint8), 130: array([1, 0, 0, 0, 0, 0, 1, 0], dtype=uint8), 131: array([1, 0, 0, 0, 0, 0, 1, 1], dtype=uint8), 132: array([1, 0, 0, 0, 0, 1, 0, 0], dtype=uint8), 133: array([1, 0, 0, 0, 0, 1, 0, 1], dtype=uint8), 134: array([1, 0, 0, 0, 0, 1, 1, 0], dtype=uint8), 135: array([1, 0, 0, 0, 0, 1, 1, 1], dtype=uint8), 136: array([1, 0, 0, 0, 1, 0, 0, 0], dtype=uint8), 137: array([1, 0, 0, 0, 1, 0, 0, 1], dtype=uint8), 138: array([1, 0, 0, 0, 1, 0, 1, 0], dtype=uint8), 139: array([1, 0, 0, 0, 1, 0, 1, 1], dtype=uint8), 140: array([1, 0, 0, 0, 1, 1, 0, 0], dtype=uint8), 141: array([1, 0, 0, 0, 1, 1, 0, 1], dtype=uint8), 142: array([1, 0, 0, 0, 1, 1, 1, 0], dtype=uint8), 143: array([1, 0, 0, 0, 1, 1, 1, 1], dtype=uint8), 144: array([1, 0, 0, 1, 0, 0, 0, 0], dtype=uint8), 145: array([1, 0, 0, 1, 0, 0, 0, 1], dtype=uint8), 146: array([1, 0, 0, 1, 0, 0, 1, 0], dtype=uint8), 147: array([1, 0, 0, 1, 0, 0, 1, 1], dtype=uint8), 148: array([1, 0, 0, 1, 0, 1, 0, 0], dtype=uint8), 149: array([1, 0, 0, 1, 0, 1, 0, 1], dtype=uint8), 150: array([1, 0, 0, 1, 0, 1, 1, 0], dtype=uint8), 151: array([1, 0, 0, 1, 0, 1, 1, 1], dtype=uint8), 152: array([1, 0, 0, 1, 1, 0, 0, 0], dtype=uint8), 153: array([1, 0, 0, 1, 1, 0, 0, 1], dtype=uint8), 154: array([1, 0, 0, 1, 1, 0, 1, 0], dtype=uint8), 155: array([1, 0, 0, 1, 1, 0, 1, 1], dtype=uint8), 156: array([1, 0, 0, 1, 1, 1, 0, 0], dtype=uint8), 157: array([1, 0, 0, 1, 1, 1, 0, 1], dtype=uint8), 158: array([1, 0, 0, 1, 1, 1, 1, 0], dtype=uint8), 159: array([1, 0, 0, 1, 1, 1, 1, 1], dtype=uint8), 160: array([1, 0, 1, 0, 0, 0, 0, 0], dtype=uint8), 161: array([1, 0, 1, 0, 0, 0, 0, 1], dtype=uint8), 162: array([1, 0, 1, 0, 0, 0, 1, 0], dtype=uint8), 163: array([1, 0, 1, 0, 0, 0, 1, 1], dtype=uint8), 164: array([1, 0, 1, 0, 0, 1, 0, 0], dtype=uint8), 165: array([1, 0, 1, 0, 0, 1, 0, 1], dtype=uint8), 166: array([1, 0, 1, 0, 0, 1, 1, 0], dtype=uint8), 167: array([1, 0, 1, 0, 0, 1, 1, 1], dtype=uint8), 168: array([1, 0, 1, 0, 1, 0, 0, 0], dtype=uint8), 169: array([1, 0, 1, 0, 1, 0, 0, 1], dtype=uint8), 170: array([1, 0, 1, 0, 1, 0, 1, 0], dtype=uint8), 171: array([1, 0, 1, 0, 1, 0, 1, 1], dtype=uint8), 172: array([1, 0, 1, 0, 1, 1, 0, 0], dtype=uint8), 173: array([1, 0, 1, 0, 1, 1, 0, 1], dtype=uint8), 174: array([1, 0, 1, 0, 1, 1, 1, 0], dtype=uint8), 175: array([1, 0, 1, 0, 1, 1, 1, 1], dtype=uint8), 176: array([1, 0, 1, 1, 0, 0, 0, 0], dtype=uint8), 177: array([1, 0, 1, 1, 0, 0, 0, 1], dtype=uint8), 178: array([1, 0, 1, 1, 0, 0, 1, 0], dtype=uint8), 179: array([1, 0, 1, 1, 0, 0, 1, 1], dtype=uint8), 180: array([1, 0, 1, 1, 0, 1, 0, 0], dtype=uint8), 181: array([1, 0, 1, 1, 0, 1, 0, 1], dtype=uint8), 182: array([1, 0, 1, 1, 0, 1, 1, 0], dtype=uint8), 183: array([1, 0, 1, 1, 0, 1, 1, 1], dtype=uint8), 184: array([1, 0, 1, 1, 1, 0, 0, 0], dtype=uint8), 185: array([1, 0, 1, 1, 1, 0, 0, 1], dtype=uint8), 186: array([1, 0, 1, 1, 1, 0, 1, 0], dtype=uint8), 187: array([1, 0, 1, 1, 1, 0, 1, 1], dtype=uint8), 188: array([1, 0, 1, 1, 1, 1, 0, 0], dtype=uint8), 189: array([1, 0, 1, 1, 1, 1, 0, 1], dtype=uint8), 190: array([1, 0, 1, 1, 1, 1, 1, 0], dtype=uint8), 191: array([1, 0, 1, 1, 1, 1, 1, 1], dtype=uint8), 192: array([1, 1, 0, 0, 0, 0, 0, 0], dtype=uint8), 193: array([1, 1, 0, 0, 0, 0, 0, 1], dtype=uint8), 194: array([1, 1, 0, 0, 0, 0, 1, 0], dtype=uint8), 195: array([1, 1, 0, 0, 0, 0, 1, 1], dtype=uint8), 196: array([1, 1, 0, 0, 0, 1, 0, 0], dtype=uint8), 197: array([1, 1, 0, 0, 0, 1, 0, 1], dtype=uint8), 198: array([1, 1, 0, 0, 0, 1, 1, 0], dtype=uint8), 199: array([1, 1, 0, 0, 0, 1, 1, 1], dtype=uint8), 200: array([1, 1, 0, 0, 1, 0, 0, 0], dtype=uint8), 201: array([1, 1, 0, 0, 1, 0, 0, 1], dtype=uint8), 202: array([1, 1, 0, 0, 1, 0, 1, 0], dtype=uint8), 203: array([1, 1, 0, 0, 1, 0, 1, 1], dtype=uint8), 204: array([1, 1, 0, 0, 1, 1, 0, 0], dtype=uint8), 205: array([1, 1, 0, 0, 1, 1, 0, 1], dtype=uint8), 206: array([1, 1, 0, 0, 1, 1, 1, 0], dtype=uint8), 207: array([1, 1, 0, 0, 1, 1, 1, 1], dtype=uint8), 208: array([1, 1, 0, 1, 0, 0, 0, 0], dtype=uint8), 209: array([1, 1, 0, 1, 0, 0, 0, 1], dtype=uint8), 210: array([1, 1, 0, 1, 0, 0, 1, 0], dtype=uint8), 211: array([1, 1, 0, 1, 0, 0, 1, 1], dtype=uint8), 212: array([1, 1, 0, 1, 0, 1, 0, 0], dtype=uint8), 213: array([1, 1, 0, 1, 0, 1, 0, 1], dtype=uint8), 214: array([1, 1, 0, 1, 0, 1, 1, 0], dtype=uint8), 215: array([1, 1, 0, 1, 0, 1, 1, 1], dtype=uint8), 216: array([1, 1, 0, 1, 1, 0, 0, 0], dtype=uint8), 217: array([1, 1, 0, 1, 1, 0, 0, 1], dtype=uint8), 218: array([1, 1, 0, 1, 1, 0, 1, 0], dtype=uint8), 219: array([1, 1, 0, 1, 1, 0, 1, 1], dtype=uint8), 220: array([1, 1, 0, 1, 1, 1, 0, 0], dtype=uint8), 221: array([1, 1, 0, 1, 1, 1, 0, 1], dtype=uint8), 222: array([1, 1, 0, 1, 1, 1, 1, 0], dtype=uint8), 223: array([1, 1, 0, 1, 1, 1, 1, 1], dtype=uint8), 224: array([1, 1, 1, 0, 0, 0, 0, 0], dtype=uint8), 225: array([1, 1, 1, 0, 0, 0, 0, 1], dtype=uint8), 226: array([1, 1, 1, 0, 0, 0, 1, 0], dtype=uint8), 227: array([1, 1, 1, 0, 0, 0, 1, 1], dtype=uint8), 228: array([1, 1, 1, 0, 0, 1, 0, 0], dtype=uint8), 229: array([1, 1, 1, 0, 0, 1, 0, 1], dtype=uint8), 230: array([1, 1, 1, 0, 0, 1, 1, 0], dtype=uint8), 231: array([1, 1, 1, 0, 0, 1, 1, 1], dtype=uint8), 232: array([1, 1, 1, 0, 1, 0, 0, 0], dtype=uint8), 233: array([1, 1, 1, 0, 1, 0, 0, 1], dtype=uint8), 234: array([1, 1, 1, 0, 1, 0, 1, 0], dtype=uint8), 235: array([1, 1, 1, 0, 1, 0, 1, 1], dtype=uint8), 236: array([1, 1, 1, 0, 1, 1, 0, 0], dtype=uint8), 237: array([1, 1, 1, 0, 1, 1, 0, 1], dtype=uint8), 238: array([1, 1, 1, 0, 1, 1, 1, 0], dtype=uint8), 239: array([1, 1, 1, 0, 1, 1, 1, 1], dtype=uint8), 240: array([1, 1, 1, 1, 0, 0, 0, 0], dtype=uint8), 241: array([1, 1, 1, 1, 0, 0, 0, 1], dtype=uint8), 242: array([1, 1, 1, 1, 0, 0, 1, 0], dtype=uint8), 243: array([1, 1, 1, 1, 0, 0, 1, 1], dtype=uint8), 244: array([1, 1, 1, 1, 0, 1, 0, 0], dtype=uint8), 245: array([1, 1, 1, 1, 0, 1, 0, 1], dtype=uint8), 246: array([1, 1, 1, 1, 0, 1, 1, 0], dtype=uint8), 247: array([1, 1, 1, 1, 0, 1, 1, 1], dtype=uint8), 248: array([1, 1, 1, 1, 1, 0, 0, 0], dtype=uint8), 249: array([1, 1, 1, 1, 1, 0, 0, 1], dtype=uint8), 250: array([1, 1, 1, 1, 1, 0, 1, 0], dtype=uint8), 251: array([1, 1, 1, 1, 1, 0, 1, 1], dtype=uint8), 252: array([1, 1, 1, 1, 1, 1, 0, 0], dtype=uint8), 253: array([1, 1, 1, 1, 1, 1, 0, 1], dtype=uint8), 254: array([1, 1, 1, 1, 1, 1, 1, 0], dtype=uint8), 255: array([1, 1, 1, 1, 1, 1, 1, 1], dtype=uint8)}\n",
      "\n",
      "\n",
      "Error:[3.45638663]\n",
      "Pred:[0 0 0 0 0 0 0 1]\n",
      "True:[0 1 0 0 0 1 0 1]\n",
      "9 + 60 = 1\n",
      "------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:[3.63389116]\n",
      "Pred:[1 1 1 1 1 1 1 1]\n",
      "True:[0 0 1 1 1 1 1 1]\n",
      "28 + 35 = 255\n",
      "------------\n",
      "Error:[3.91366595]\n",
      "Pred:[0 1 0 0 1 0 0 0]\n",
      "True:[1 0 1 0 0 0 0 0]\n",
      "116 + 44 = 72\n",
      "------------\n",
      "Error:[3.72191702]\n",
      "Pred:[1 1 0 1 1 1 1 1]\n",
      "True:[0 1 0 0 1 1 0 1]\n",
      "4 + 73 = 223\n",
      "------------\n",
      "Error:[3.5852713]\n",
      "Pred:[0 0 0 0 1 0 0 0]\n",
      "True:[0 1 0 1 0 0 1 0]\n",
      "71 + 11 = 8\n",
      "------------\n",
      "Error:[2.53352328]\n",
      "Pred:[1 0 1 0 0 0 1 0]\n",
      "True:[1 1 0 0 0 0 1 0]\n",
      "81 + 113 = 162\n",
      "------------\n",
      "Error:[0.57691441]\n",
      "Pred:[0 1 0 1 0 0 0 1]\n",
      "True:[0 1 0 1 0 0 0 1]\n",
      "81 + 0 = 81\n",
      "------------\n",
      "Error:[1.42589952]\n",
      "Pred:[1 0 0 0 0 0 0 1]\n",
      "True:[1 0 0 0 0 0 0 1]\n",
      "4 + 125 = 129\n",
      "------------\n",
      "Error:[0.47477457]\n",
      "Pred:[0 0 1 1 1 0 0 0]\n",
      "True:[0 0 1 1 1 0 0 0]\n",
      "39 + 17 = 56\n",
      "------------\n",
      "Error:[0.21595037]\n",
      "Pred:[0 0 0 0 1 1 1 0]\n",
      "True:[0 0 0 0 1 1 1 0]\n",
      "11 + 3 = 14\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "import copy, numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "# compute sigmoid nonlinearity\n",
    "def sigmoid(x):\n",
    "    output = 1/(1+np.exp(-x))\n",
    "    return output\n",
    "\n",
    "# convert output of sigmoid function to its derivative\n",
    "def sigmoid_output_to_derivative(output):\n",
    "    return output*(1-output)\n",
    "\n",
    "\n",
    "# training dataset generation\n",
    "int2binary = {}\n",
    "binary_dim = 8\n",
    "\n",
    "largest_number = pow(2,binary_dim)\n",
    "binary = np.unpackbits(\n",
    "    np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
    "for i in range(largest_number):\n",
    "    int2binary[i] = binary[i]\n",
    "\n",
    "print(\"---binary---\")\n",
    "print(binary)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"---int2binary---\")\n",
    "print(int2binary)\n",
    "print(\"\\n\")\n",
    "\n",
    "# input variables\n",
    "alpha = 0.1\n",
    "input_dim = 2\n",
    "hidden_dim = 16\n",
    "output_dim = 1\n",
    "\n",
    "\n",
    "# initialize neural network weights\n",
    "synapse_0 = 2*np.random.random((input_dim,hidden_dim)) - 1\n",
    "synapse_1 = 2*np.random.random((hidden_dim,output_dim)) - 1\n",
    "synapse_h = 2*np.random.random((hidden_dim,hidden_dim)) - 1\n",
    "\n",
    "synapse_0_update = np.zeros_like(synapse_0)\n",
    "synapse_1_update = np.zeros_like(synapse_1)\n",
    "synapse_h_update = np.zeros_like(synapse_h)\n",
    "\n",
    "# training logic\n",
    "for j in range(10000):\n",
    "    \n",
    "    # generate a simple addition problem (a + b = c)\n",
    "    a_int = np.random.randint(largest_number/2) # int version\n",
    "    a = int2binary[a_int] # binary encoding\n",
    "\n",
    "    b_int = np.random.randint(largest_number/2) # int version\n",
    "    b = int2binary[b_int] # binary encoding\n",
    "\n",
    "    # true answer\n",
    "    c_int = a_int + b_int\n",
    "    c = int2binary[c_int]\n",
    "    \n",
    "    # where we'll store our best guess (binary encoded)\n",
    "    d = np.zeros_like(c)\n",
    "\n",
    "    overallError = 0\n",
    "    \n",
    "    layer_2_deltas = list()\n",
    "    layer_1_values = list()\n",
    "    layer_1_values.append(np.zeros(hidden_dim))\n",
    "    \n",
    "    # moving along the positions in the binary encoding\n",
    "    for position in range(binary_dim):\n",
    "        \n",
    "        # generate input and output\n",
    "        X = np.array([[a[binary_dim - position - 1],b[binary_dim - position - 1]]])\n",
    "        y = np.array([[c[binary_dim - position - 1]]]).T\n",
    "\n",
    "        # hidden layer (input ~+ prev_hidden)\n",
    "        layer_1 = sigmoid(np.dot(X,synapse_0) + np.dot(layer_1_values[-1],synapse_h))\n",
    "\n",
    "        # output layer (new binary representation)\n",
    "        layer_2 = sigmoid(np.dot(layer_1,synapse_1))\n",
    "\n",
    "        # did we miss?... if so by how much?\n",
    "        layer_2_error = y - layer_2\n",
    "        layer_2_deltas.append((layer_2_error)*sigmoid_output_to_derivative(layer_2))\n",
    "        overallError += np.abs(layer_2_error[0])\n",
    "    \n",
    "        # decode estimate so we can print it out\n",
    "        d[binary_dim - position - 1] = np.round(layer_2[0][0])\n",
    "        \n",
    "        # store hidden layer so we can use it in the next timestep\n",
    "        layer_1_values.append(copy.deepcopy(layer_1))\n",
    "    \n",
    "    future_layer_1_delta = np.zeros(hidden_dim)\n",
    "    \n",
    "    for position in range(binary_dim):\n",
    "        \n",
    "        X = np.array([[a[position],b[position]]])\n",
    "        layer_1 = layer_1_values[-position-1]\n",
    "        prev_layer_1 = layer_1_values[-position-2]\n",
    "        \n",
    "        # error at output layer\n",
    "        layer_2_delta = layer_2_deltas[-position-1]\n",
    "        # error at hidden layer\n",
    "        layer_1_delta = (future_layer_1_delta.dot(synapse_h.T) + \\\n",
    "            layer_2_delta.dot(synapse_1.T)) * sigmoid_output_to_derivative(layer_1)\n",
    "        # let's update all our weights so we can try again\n",
    "        synapse_1_update += np.atleast_2d(layer_1).T.dot(layer_2_delta)\n",
    "        synapse_h_update += np.atleast_2d(prev_layer_1).T.dot(layer_1_delta)\n",
    "        synapse_0_update += X.T.dot(layer_1_delta)\n",
    "        \n",
    "        future_layer_1_delta = layer_1_delta\n",
    "    \n",
    "\n",
    "    synapse_0 += synapse_0_update * alpha\n",
    "    synapse_1 += synapse_1_update * alpha\n",
    "    synapse_h += synapse_h_update * alpha    \n",
    "\n",
    "    synapse_0_update *= 0\n",
    "    synapse_1_update *= 0\n",
    "    synapse_h_update *= 0\n",
    "    \n",
    "    # print out progress\n",
    "    if(j % 1000 == 0):\n",
    "        print(\"Error:\" + str(overallError))\n",
    "        print(\"Pred:\" + str(d))\n",
    "        print(\"True:\" + str(c))\n",
    "        out = 0\n",
    "        for index,x in enumerate(reversed(d)):\n",
    "            out += x*pow(2,index)\n",
    "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out))\n",
    "        print(\"------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow Python 3",
   "language": "python",
   "name": "tensorflowpython3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
